Task GDG Research 
Vatsal Jain
The Article Why language models hallucinate is a concrete and fundamental explanation on how and why do LLMs Hallucinate. Hallucination is when ai models generate wrong or incorrect answers with confidence, for example as mentioned in the article they will predict birthdays wrong with confidence, ideally just answer the question and no accepting that they don’t have the knowledge to answer the question , this is because that is how they are trained , the main metric that they are evaluated is accuracy , how many right answer they can get , to explain in easy words and how we can related to this is as in an entrance exam MCQ based even if we don’t know the answer we will guess randomly to get more marks , likewise ai also guesses when they don’t know , so what openai is focusing on to solve hallucinations is that make the wrong guess penalize more and give partial credit for abstention saying “I don’t know”, just like in negative marking exam you won’t guess answers because they is penalty for wrong answers , and also it will always be true that AI will never achieve 100% accuracy because it is not possible to know everything.
I personally think that ai hallucinations and other drawbacks of certain technologies exist because there is too much competition and everyone is competing on the wrong metrics, I feel this points out a very core advantage what researcher have due to their setting or environment they are not bound to generate revenue and are free to explore and use their creativity to tackle challenges , also the grow in ai was possible due to open source contributions by companies as well as researchers.
Transformers is a very interesting domain I would definitely explore it gives the ai model to ability of read all things at once and remove the long problem of reading one word by one in sequential order, this is how transformers changed the way AI perform and train , like wise this research paper by open ai on Why language models hallucinate will solve and redefine some fundamental evaluation metrics and make ai better the report highlights the significate improve in gpt 5 in terms of hallucinations.